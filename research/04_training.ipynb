{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\User\\\\Desktop\\\\PROJECTS\\\\Student-predictor\\\\student app\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\User\\\\Desktop\\\\PROJECTS\\\\Student-predictor\\\\student app'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    train_df: Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    training_data: Path\n",
    "    trained_model_path: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from studentApp.constants import *\n",
    "from studentApp.utils.common import read_yaml, create_directories\n",
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            train_df=Path(config.train_df),\n",
    "          \n",
    "        )\n",
    "\n",
    "        return  data_transformation_config\n",
    "\n",
    " \n",
    "    def get_training_config(self):\n",
    "        training = self.config.training\n",
    "        training_data_path = 'artifacts/data_ingestion/train.csv'  # Replace with the correct file path\n",
    "        training_root_dir = 'artifacts'  # Replace with the desired root directory path\n",
    "\n",
    "        create_directories([Path(training_root_dir)])\n",
    "\n",
    "        # Load the training data using pd.read_csv\n",
    "        training_data_df = pd.read_csv(training_data_path)\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training_root_dir),\n",
    "            training_data=training_data_df,  # Use the DataFrame directly as the training data\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "\n",
    "            # Add other configuration attributes as needed\n",
    "        )\n",
    "\n",
    "        return training_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from studentApp.constants import *\n",
    "from studentApp.utils.common import read_yaml, create_directories\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.train_df = None\n",
    "        self.train = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def _create_preprocessor(self):\n",
    "        numerical_columns = [\"writing_score\", \"reading_score\"]\n",
    "        categorical_columns = [\n",
    "            \"gender\",\n",
    "            \"race_ethnicity\",\n",
    "            \"parental_level_of_education\",\n",
    "            \"lunch\",\n",
    "            \"test_preparation_course\",\n",
    "        ]\n",
    "\n",
    "        num_pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        cat_pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"one_hot_encoder\", OneHotEncoder()),\n",
    "                (\"scaler\", StandardScaler(with_mean=False)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num_pipeline\", num_pipeline, numerical_columns),\n",
    "                (\"cat_pipeline\", cat_pipeline, categorical_columns),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "    def transformation_data(self):\n",
    "        train_df = pd.read_csv(self.config.train_df)\n",
    "\n",
    "        X = train_df.drop(columns=['math_score'], axis=1)\n",
    "        y = train_df['math_score']\n",
    "\n",
    "        preprocessor = self._create_preprocessor()\n",
    "\n",
    "        self.X = preprocessor.fit_transform(X)\n",
    "        self.y = y\n",
    "\n",
    "        print(\"Data transformation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Training:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.models = {\n",
    "            \"Linear Regression\": LinearRegression(),\n",
    "            \"Lasso\": Lasso(),\n",
    "            \"Ridge\": Ridge(),\n",
    "            \"K-Neighbors Regressor\": KNeighborsRegressor(),\n",
    "            \"Decision Tree\": DecisionTreeRegressor(),\n",
    "            \"Random Forest Regressor\": RandomForestRegressor(),\n",
    "            \"XGBRegressor\": XGBRegressor(),\n",
    "            \"CatBoosting Regressor\": CatBoostRegressor(verbose=False),\n",
    "            \"AdaBoost Regressor\": AdaBoostRegressor()\n",
    "        }\n",
    "        self.metrics = {\n",
    "            \"Mean Squared Error\": mean_squared_error,\n",
    "            \"R2 Score\": r2_score\n",
    "        }\n",
    "\n",
    "    def save_model(self, model_name, model):\n",
    "        save_directory = Path(\"artifacts\")\n",
    "        save_path = save_directory / \"model.pkl\"\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"The best model '{model_name}' has been saved.\")\n",
    "\n",
    "    def evaluate_model(self, model, X, y):\n",
    "        scores = {}\n",
    "        for metric_name, metric_func in self.metrics.items():\n",
    "            y_pred = model.predict(X)\n",
    "            score = metric_func(y, y_pred)\n",
    "            scores[metric_name] = score\n",
    "        return scores\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        best_model = None\n",
    "        best_model_name = \"\"\n",
    "        best_scores = None\n",
    "        total_models = len(self.models)\n",
    "        current_model = 1\n",
    "\n",
    "        for model_name, model in self.models.items():\n",
    "            model.fit(X_train, y_train)  # Train model\n",
    "\n",
    "            # Evaluate the model using multiple metrics\n",
    "            scores = self.evaluate_model(model, X_train, y_train)\n",
    "\n",
    "            if best_scores is None or all(score > best_scores[metric] for metric, score in scores.items()):\n",
    "                best_scores = scores\n",
    "                best_model_name = model_name\n",
    "                best_model = model\n",
    "\n",
    "            # Display metrics for all models\n",
    "            print(f\"Model: {model_name}\")\n",
    "            for metric, score in scores.items():\n",
    "                print(f\"- {metric}: {score:.4f}\")\n",
    "\n",
    "            # Calculate and display percentage progress\n",
    "            progress = current_model / total_models * 100\n",
    "            print(f\"Training Progress: {progress:.2f}%\")\n",
    "            current_model += 1\n",
    "\n",
    "        if best_model is not None:\n",
    "            self.save_model(best_model_name, best_model)\n",
    "            print(\"\\nBest Model Scores:\")\n",
    "            for metric, score in best_scores.items():\n",
    "                print(f\"- {metric}: {score:.4f}\")\n",
    "        else:\n",
    "            print(\"No best model found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-20 19:22:32,707: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-07-20 19:22:32,711: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-07-20 19:22:32,715: INFO: common: created directory at: artifacts]\n",
      "[2023-07-20 19:22:32,719: INFO: common: created directory at: artifacts/data_ingestion]\n",
      "Data transformation complete\n",
      "[2023-07-20 19:22:32,808: INFO: common: created directory at: artifacts]\n",
      "Model: Linear Regression\n",
      "- Mean Squared Error: 28.2157\n",
      "- R2 Score: 0.8780\n",
      "Training Progress: 11.11%\n",
      "Model: Lasso\n",
      "- Mean Squared Error: 34.4055\n",
      "- R2 Score: 0.8513\n",
      "Training Progress: 22.22%\n",
      "Model: Ridge\n",
      "- Mean Squared Error: 28.1649\n",
      "- R2 Score: 0.8783\n",
      "Training Progress: 33.33%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: K-Neighbors Regressor\n",
      "- Mean Squared Error: 70.3507\n",
      "- R2 Score: 0.6959\n",
      "Training Progress: 44.44%\n",
      "Model: Decision Tree\n",
      "- Mean Squared Error: 0.0147\n",
      "- R2 Score: 0.9999\n",
      "Training Progress: 55.56%\n",
      "Model: Random Forest Regressor\n",
      "- Mean Squared Error: 5.0686\n",
      "- R2 Score: 0.9781\n",
      "Training Progress: 66.67%\n",
      "Model: XGBRegressor\n",
      "- Mean Squared Error: 0.9952\n",
      "- R2 Score: 0.9957\n",
      "Training Progress: 77.78%\n",
      "Model: CatBoosting Regressor\n",
      "- Mean Squared Error: 9.8131\n",
      "- R2 Score: 0.9576\n",
      "Training Progress: 88.89%\n",
      "Model: AdaBoost Regressor\n",
      "- Mean Squared Error: 32.4788\n",
      "- R2 Score: 0.8596\n",
      "Training Progress: 100.00%\n",
      "The best model 'Linear Regression' has been saved.\n",
      "\n",
      "Best Model Scores:\n",
      "- Mean Squared Error: 28.2157\n",
      "- R2 Score: 0.8780\n"
     ]
    }
   ],
   "source": [
    "# Integration of DataTransformation and Training classes\n",
    "try:\n",
    "\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.transformation_data()\n",
    "\n",
    "    \n",
    "    # data_transformation = DataTransformation(config)  # Create an instance of DataTransformation\n",
    "    # data_transformation.transformation_data()  # Perform data transformation\n",
    "\n",
    "    X_train = data_transformation.X\n",
    "    y_train = data_transformation.y\n",
    "    \n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    # training = Training()  # Create an instance of Training\n",
    "    training.train(X_train, y_train) \n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(850, 19)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "# try:\n",
    "\n",
    "  \n",
    "\n",
    "#     training_config = config.get_training_config()\n",
    "#     training = Training(config=training_config)\n",
    "#     training.train_generator()\n",
    "#     training.train(\n",
    "#         callback_list=callback_list\n",
    "#     )\n",
    "    \n",
    "# except Exception as e:\n",
    "#     raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (850, 19), indices imply (850, 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\internals\\managers.py:1671\u001b[0m, in \u001b[0;36mcreate_block_manager_from_blocks\u001b[1;34m(blocks, axes)\u001b[0m\n\u001b[0;32m   1666\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1667\u001b[0m         \u001b[39m# It's OK if a single block is passed as values, its placement\u001b[39;00m\n\u001b[0;32m   1668\u001b[0m         \u001b[39m# is basically \"all items\", but if there're many, don't bother\u001b[39;00m\n\u001b[0;32m   1669\u001b[0m         \u001b[39m# converting, it's an error anyway.\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m         blocks \u001b[39m=\u001b[39m [\n\u001b[1;32m-> 1671\u001b[0m             make_block(values\u001b[39m=\u001b[39;49mblocks[\u001b[39m0\u001b[39;49m], placement\u001b[39m=\u001b[39;49m\u001b[39mslice\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39mlen\u001b[39;49m(axes[\u001b[39m0\u001b[39;49m])))\n\u001b[0;32m   1672\u001b[0m         ]\n\u001b[0;32m   1674\u001b[0m mgr \u001b[39m=\u001b[39m BlockManager(blocks, axes)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\internals\\blocks.py:2744\u001b[0m, in \u001b[0;36mmake_block\u001b[1;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[0;32m   2742\u001b[0m     values \u001b[39m=\u001b[39m DatetimeArray\u001b[39m.\u001b[39m_simple_new(values, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m-> 2744\u001b[0m \u001b[39mreturn\u001b[39;00m klass(values, ndim\u001b[39m=\u001b[39;49mndim, placement\u001b[39m=\u001b[39;49mplacement)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\internals\\blocks.py:130\u001b[0m, in \u001b[0;36mBlock.__init__\u001b[1;34m(self, values, placement, ndim)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_ndim \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmgr_locs) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues):\n\u001b[1;32m--> 130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    131\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrong number of items passed \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mplacement implies \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmgr_locs)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong number of items passed 19, placement implies 7",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Assuming you have defined the preprocessor as shown in the previous code\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# preprocessor = ...\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[39m# Convert X_train to a DataFrame\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m X_train_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(X_train, columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mwriting_score\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mreading_score\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgender\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrace_ethnicity\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mparental_level_of_education\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlunch\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtest_preparation_course\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     10\u001b[0m \u001b[39m# Create an instance of CustomData with the input data\u001b[39;00m\n\u001b[0;32m     11\u001b[0m data \u001b[39m=\u001b[39m CustomData(\n\u001b[0;32m     12\u001b[0m     gender\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFemale\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     race_ethnicity\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGroup A\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     writing_score\u001b[39m=\u001b[39m\u001b[39m85\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\frame.py:497\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    495\u001b[0m         mgr \u001b[39m=\u001b[39m init_dict({data\u001b[39m.\u001b[39mname: data}, index, columns, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    496\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m         mgr \u001b[39m=\u001b[39m init_ndarray(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    499\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, abc\u001b[39m.\u001b[39mIterable) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m)):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\internals\\construction.py:234\u001b[0m, in \u001b[0;36minit_ndarray\u001b[1;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m     block_values \u001b[39m=\u001b[39m [values]\n\u001b[1;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m create_block_manager_from_blocks(block_values, [columns, index])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\internals\\managers.py:1681\u001b[0m, in \u001b[0;36mcreate_block_manager_from_blocks\u001b[1;34m(blocks, axes)\u001b[0m\n\u001b[0;32m   1679\u001b[0m blocks \u001b[39m=\u001b[39m [\u001b[39mgetattr\u001b[39m(b, \u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m, b) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m blocks]\n\u001b[0;32m   1680\u001b[0m tot_items \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(b\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m blocks)\n\u001b[1;32m-> 1681\u001b[0m \u001b[39mraise\u001b[39;00m construction_error(tot_items, blocks[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:], axes, e)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (850, 19), indices imply (850, 7)"
     ]
    }
   ],
   "source": [
    "from studentApp.pipeline.predict import CustomData, PredictPipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have defined the preprocessor as shown in the previous code\n",
    "# preprocessor = ...\n",
    "\n",
    "# Convert X_train to a DataFrame\n",
    "X_train_df = pd.DataFrame(X_train, columns=[\"writing_score\", \"reading_score\", \"gender\", \"race_ethnicity\", \"parental_level_of_education\", \"lunch\", \"test_preparation_course\"])\n",
    "\n",
    "# Create an instance of CustomData with the input data\n",
    "data = CustomData(\n",
    "    gender=\"Female\",\n",
    "    race_ethnicity=\"Group A\",\n",
    "    parental_level_of_education=\"Bachelor's Degree\",\n",
    "    lunch=\"Standard\",\n",
    "    test_preparation_course=\"Completed\",\n",
    "    reading_score=80,\n",
    "    writing_score=85\n",
    ")\n",
    "\n",
    "# Get the data as a DataFrame\n",
    "pred_df = data.get_data_as_data_frame()\n",
    "\n",
    "# Create an instance of PredictPipeline\n",
    "predict_pipeline = PredictPipeline()\n",
    "\n",
    "# Fit the preprocessor with the training data DataFrame\n",
    "predict_pipeline.fit_preprocessor(X_train_df)\n",
    "\n",
    "# Perform the prediction\n",
    "# Before making predictions, we need to ensure pred_df has the same columns as X_train_df\n",
    "# Reorder and select the columns in the same order as during training\n",
    "pred_df_transformed = pred_df[X_train_df.columns]\n",
    "\n",
    "results_transformed = predict_pipeline.predict(pred_df_transformed)\n",
    "\n",
    "# Perform inverse transformation on the prediction results\n",
    "results_original = predict_pipeline.inverse_transform(results_transformed)\n",
    "\n",
    "# Print the prediction results in their original form\n",
    "print(\"Prediction Results:\")\n",
    "print(results_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "student",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
